---
layout: post
title: "[Notes] WSOD"
excerpt: "Weakly-Supervised Object Detection. Sounds cool, right?"
---

I've been exploring a lot of "low-resource" learning recently. This note covers the CVPR'18 tutorial on WSOD. Akshay Chawla and I are working on a *related idea*... 

References:
- Tutorial 2018: https://hbilen.github.io/wsl-cvpr18.github.io/assets/wsod.pdf
- The OG paper on WSDDN (Weakly Supervised Deep **Detection** Networks) 2016 : https://arxiv.org/pdf/1511.02853.pdf
- An equally interesting approach to object detection using reinforcement learning. 2016 - http://www.maths.lth.se/sminchisescu/media/papers/mps-rlcvpr16.pdf

---

## WSOD cvpr 2018 tutorial
Author: Hakan Bilen

- Given data : image and weak annotation (which object is present, but not **WHERE**)
- What's the least we can say given positive image?
    - At least one object in the image is from the positive class.
    - For negative images, no object is from the target class.

Standard approach is MIL find the true-positive windows by training a window classifier. Positive instances(or windows) can be taken from positive bags(images) and negative instances can be taken(*sampled?*) from the negative bags.

This follows a alternating optimisation approach.
- Initialize positive/negative instances.
    - Either the full image or removing a margin (Question: how much margin? run a hyperparameter search)
- Propose bags
    - Create by sliding window or object proposals
- Re-localize and re-train object detectors.
    - Re-localization objective: $\underset{b}{argmax} A(x_b)$, where A is an *appearance* model. This should select the best box/localisation or proposal? (Possible issue: you might end up losing out on "good" b.boxes when you select just one bbox per image as TP, there may be multiple TP)
    - Re-training detectors: Max-margin type loss for positive and negative instances. Different from supervised-learning. Check RCNN and FasterRCC.

- Recent work relaxes the max opearator in the detection to a softmax type score for the proposals. (This is interesting, basically it is ranking proposals by using this "softmax" score, a higher softmax score represents a good tradeoff between tightness/object features?)

**Re-visit **slide 21** on Self-Paced Learning**:
- Selection of samples via confidence of max scoring window [Kumar NIPS 10]
- Selection of window space by allowing smaller windows [Bilen IJCV 14, Shi ECCV 14]
- Selection of samples via intercategory competition [Sangineto PAMI 17]
- More robust re-localization: Multifold MIL [Cinbis CVPR 14]

### Eliminating bad priors.
For N positive images, each with W positive windows, we will have to consider $W^N$ samples. But many of them will be very bad samples. Can we eliminate? Tutorial covers many different ideas, but ***OBJECTNESS*** seems promising.
- How likely is a window to contain **any** object of **any** class?
- This focusses the localization on the objects and not on the background. The original YOLO also employs something similar - the prior probability is simply that this bounding-box contains an object; the posterior is the probability of the target classes given that bounding-box crop.
- *Should* push towards whole objects instead of just sub-regions.
    - (original) objectness paper CVPR'10 - http://groups.inf.ed.ac.uk/calvin/Publications/alexe-cvpr10.pdf
- Mutual exclusion [Bilen BMVC 14] is another good idea
    - Assumption: A box can tightly cover only one object instance
-  Scale [Shi ECCV 16]
    -  Weight object proposals according to estimated size

## Weakly Supervised Deep Detection Networks
Authors: Bilen, Vedaldi
Year: 2016

### Introduction

Paper explores WSD (weakly supervised detection) with only image-level labels. Prior work exists but not a single e2e architecture.

1. Given an image $x$, extract region level descriptors $\phi(x; R)$ by inserting a spatial pyramid pooling layer on top of the CNN layers.
2. Extract two branches from the pooled region-level features
    - First stream will give class score $\phi^c(x; R)$ to every region. This is **recognition**.
    - Second stream will *compare* regions, by computing a distrubution $\phi^d(x; R)$ over them. This is **detection**.
    - Since the "region" for both streams is same, the second stream represents the region with the best features of the image (else it wouldn't be classified correctly).
    - **Worth expanding on the detection stream - regularizing on perhaps the size of the bbox.**
3. These scores are then combined to predict the image-level class scores, which is where the "supervision" comes from.
    - Combination done using "bilinear pooling" (or at least that's what I think it's called). They take the outer product of the two final representations.
    - **Worth expanding on this final combination. This crude dot-product was one of the approaches used in VQA. Worth looking at Mannning's MACNet architecture.**

Paper achieves SOTA for WSOD on PASCAL-VOC.

### Related Work

- Prior approaches approach as MIL (discussed above).
    - Bag (entire image) consists of instances (regions) of which one/some will lead to a high classification score (i.e. will contain the actual object).
    - Alternate optimisation b/w maximizing the object appearance and then selecting the right regions.
- These approaches would often get stuch in local-minima.

### Method

![](https://i.imgur.com/Damb6S7.png)

**Recognition Stream**
- Standard softmax is computed for EVERY REGION, for EVERY class probability. The final output after applying a linear map is $\textbf{x}^c$ matrix of size $C \times |R|$.

**Detection Stream**
- This part is not very clear. The paper (Section 3.2) says that there'a linear map after the last FC layer. This maps it to $\textbf{x}^d$ size $C \times |R|$ == num_classes $\times$ num_regions.
- The difference from **Recognition** is that softmax scores are computed *class* wise for all regions (i.e. denominator sums over all regions)

**Combination**
- ELEMENT WISE product is taken as opposed to outer-product. Remember that $\textbf{x}^c$ and $\textbf{x}^d$ are matrices.
- Non-maxima suppression is performed to obtain final list of *CLASS-SPECIFIC* detection regions in an image.
- Final region level scores are then summed over every class - again, not clear, but this HAS to be done because the supervision is at an image level.

**Training**
Loss function is some *form* of log-loss. Not clear. The $log$ is outside for some reason: $\sum log(y_{ki}(\phi^y_k(x_i | w)))$. Will have to check this...

### Experiments
Results on PASCAL VOC 2007, 2010 datasets.