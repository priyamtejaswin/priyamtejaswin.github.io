---
layout: post
title: "Revisiting AEVB"
excerpt: "Old dog, new tricks."
---

I went back to reading about TrueSkill, with some vague ideas about adding things like home-advantage, injuries, etc. I know first-hand how painful it can be to derive EP updates manually. Even for relatively "trivial" problems, it just takes too long. Instead, I started searching for black-box inference techniques. I remembered having played around with PyMC3, and that is supported a framework called ADVI. And that's where I started -- with the ADVI paper. As I worked my way down the rabbit hole of references, I came across AEVB.

## Setup
You have dataset $X$ consisting of $N$ i.i.d. samples. Each point $x^i$ can be continuous or discrete.

Data is generated in a 2 step process:
1. A value $z^i$ is sampled from the latent variable $z$ with probability $p(z)$.
2. A value $x^i$ is generated according to conditional probability distribution $p(x \mid z)$.

$z$ is unseen; all parameters in the densities are unknown; if $p(x \mid z)$ is a neural network, then its weights are the parameters $\theta$.

You want to estimate the parameters, and the latent variables.

Unlike some other approaches, we make *NO* simplifying assumptions about the posterior $p(z \mid x)$, or the marginal $p(x)$.

Interested in general algo that works in the following cases:
1. Intractability: marginal $p(x)$ is intractable. Posterior is intractable. And any integrals required for mean-field approaches are also intractable.
2. Large datasest: full batch optimization is not possible. Would like to work with small mini-batches. 

$$
p(z \mid x) = \frac{p(x \mid z) p(z)}{p(x)}
$$

$p(z \mid x)$ is intractable. Introduce an approximation of the true posterior $q_{\phi}(z \mid x)$. In mean-field VI, you ensure that $q_{\phi}$ can be factorized into some well-known distributions which are tractable (i.e. closed form updates). We assume the form is intractable.

## Why Encoder-Decoder?
From coding theory perspective, $z$ can be interpreted as a latent representation, or a *code*. Hence, the approximation $p(z \mid x)$ can be thought of as an *encoder* -- given datapoint $x$, the encoder generates a distribution over values of $z$ from which the datapoint $x$ could have been generated. Similarly, we refer to $p(x \mid z)$ as a *decoder* that generates a distribution over $x$.

In effect, we are trying to estimate the parameters and the latent variables by:
1. Estimating the (distribution over) the lantent variables.
2. Sampling from the latent distribution.
3. Reconstructing the sample using the likelihood $p(x \mid z)$

## ELBO
We want to find $q_\phi$ that is closest to $p_{\theta}(z \mid x)$. KL divergence gives is one measure. But minimizing it directly is not possible.

<div class="post-image">
<img src="/assets/images/aevb-elbo.png">
<p><em><font size="-1">ELBO reformulation.</font></em></p>
</div>

We only care about Eq.3. Expectation is w.r.t $q_\phi$: $\sum_{z} q_\phi (z \mid x)\ \text{log}\ p_\theta(x \mid z)$.
How to interpret this formulation?

**$x$ is observed, and $z$ is sampled**

The 2nd term is log-likelihood of the given input, for the $z$ that we have sampled. This term will be maximized only when the highest probability is assigned to the original/true value of $x$. Think of this as the *reconstruction error*.

The first term (KL divergence) is a form of a regularizer. Think back to the normal non-probabilistic auto-encoder.

<div class="post-image">
<img src="/assets/images/aevb-autoenc.png">
<p><em><font size="-1">Normal autoencoder.</font></em></p>
</div>

There is a chance that the network learns to *copy* the input in the *code*. To combat such overfitting, people proposed a number of solutions:
* Limit the number of units -- forced to learn only the most representative features.
* Corrupt the input, but use the original in the reconstruction -- downsample, noise, etc.

This term forces the encoding to be similar to the prior distribution -- if your prior $p(z)$ is a Normal, then this will force the codes to resemble Normals.

There are also some other advantages of sampling:
* It automatically acts as a noise inducer -- because you expect similar outputs from nearby samples of a code.
* This is what also ensure a smooth interpolation between two points in the codes space.

## SGVB estimator
We want to take derivatives of the ELBO w.r.t $\theta, \phi$ and then optimise using SGD, Adam etc.
One issue is that $z$ is still a random variable : $z \sim q_\phi(z \mid x)$. The paper proposes tranform to a differentiable function:

<div class="post-image">
<img src="/assets/images/aevb-trick.png">
<p><em><font size="-1">Transform.</font></em></p>
</div>

This leads to the following reformulation:

<div class="post-image">
<img src="/assets/images/aevb-samples.png">
<p><em><font size="-1">ELBO with sampling.</font></em></p>
</div>

which can be used with mini-batch estimates, and a single sample given a large batch:

<div class="post-image">
<img src="/assets/images/aevb-minibatch.png">
<p><em><font size="-1">Minibatch estimator.</font></em></p>
</div>

<div class="post-image">
<img src="/assets/images/aevb-algo.png">
<p><em><font size="-1">AEVB algo.</font></em></p>
</div>


## Re-parameterization Trick
In some conditions, it is possible to express random variable z as a deterministic variable z = g(e, x), where $g$ is parameterized by phi.

<div class="post-image">
<img src="/assets/images/aevb-gauss.png">
<p><em><font size="-1">Normal aux var for Reparameterization Trick.</font></em></p>
</div>

## Things I skipped ...
* Other ELBO estimators, and their tradeoffs. There is a estimator that does not include KL divergence. Another one does not use the Reparameterization Trick, and has higher variance.
* Suggestions on the deterministic differentiable transform function -- you can use more than just Gaussians as priors.
* Form of the encoder has a diagonal covariance -- assumes all "codes" are independent. This might make some sense for simplistic datasets like imagenet: code0 represents number, code1 represents size, code2 represents thickness, code3 represents tilt, etc ... But might not make sense for others. This also simplifies computation -- encoder output is just 2xK, where K is dim size of z -- first K values are the means, and next K are the std deviations.
* This only works for continuous latent variables -- as with SGD. Paper mentions 1 other paper that has comparable time-complexity and is also applicable for discrete vars.
